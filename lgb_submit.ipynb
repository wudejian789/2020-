{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from numpy.lib.stride_tricks import as_strided as stride\n",
    "from sklearn.model_selection import GroupKFold,StratifiedKFold,GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import random,gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xyt三维特征\n",
    "def neighbor3d_feature(group):\n",
    "    thresholds = [24,48,96]\n",
    "    dist = np.zeros((group.shape[0],group.shape[0]), dtype='float32')\n",
    "    tmp = group[['x','y','t']].values.astype('float32').reshape(-1,3)\n",
    "    dx,dy,dt = (tmp[:,0].reshape(-1,1)-tmp[:,0].reshape(1,-1))**2,(tmp[:,1].reshape(-1,1)-tmp[:,1].reshape(1,-1))**2,(tmp[:,2].reshape(-1,1)-tmp[:,2].reshape(1,-1))**2\n",
    "    dist = np.sqrt(dx+dy+dt)\n",
    "    tmps = []\n",
    "    for t in thresholds:\n",
    "        isInBall = dist<=t\n",
    "        num = dist.sum(axis=1)\n",
    "        q = group['q'].values.reshape(1,-1) * isInBall\n",
    "        d = dist * isInBall\n",
    "        n = isInBall.sum(axis=1)\n",
    "        tmps += [n, d.sum(axis=1)/n, q.sum(axis=1)/n, q.max(axis=1)]\n",
    "        q[q==0],d[d==0] = np.inf,np.inf\n",
    "        dmin = d.min(axis=1)\n",
    "        dmin[dmin==np.inf] = -1\n",
    "        tmps += [dmin, q.min(axis=1)]\n",
    "    return np.hstack([i.reshape(-1,1) for i in tmps])\n",
    "\n",
    "# xy二维特征\n",
    "def neighbor2d_feature(group):\n",
    "    thresholds = [10*np.sqrt(2), 20*np.sqrt(2), 40*np.sqrt(2)]\n",
    "    dist = np.zeros((group.shape[0],group.shape[0]), dtype='float32')\n",
    "    tmp = group[['x','y']].values.astype('float32').reshape(-1,2)\n",
    "    dx,dy = (tmp[:,0].reshape(-1,1)-tmp[:,0].reshape(1,-1))**2,(tmp[:,1].reshape(-1,1)-tmp[:,1].reshape(1,-1))**2\n",
    "    dist = np.sqrt(dx+dy)\n",
    "    tmps = []\n",
    "    for t in thresholds:\n",
    "        isInBall = dist<=t\n",
    "        num = dist.sum(axis=1)\n",
    "        q = group['q'].values.reshape(1,-1) * isInBall\n",
    "        d = dist * isInBall\n",
    "        n = isInBall.sum(axis=1)\n",
    "        tmps += [n, d.sum(axis=1)/n, q.sum(axis=1)/n, q.max(axis=1)]\n",
    "        q[q==0],d[d==0] = np.inf,np.inf\n",
    "        dmin = d.min(axis=1)\n",
    "        dmin[dmin==np.inf] = -1\n",
    "        tmps += [dmin, q.min(axis=1)]\n",
    "    return np.hstack([i.reshape(-1,1) for i in tmps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征提取函数\n",
    "def feature_extract(data):\n",
    "    out = pd.DataFrame(dtype='float32')\n",
    "    # base feature\n",
    "    print('Start to extract the base features...')\n",
    "    \n",
    "#  初赛结束后新加的部分特征，线下结果有较大提升    \n",
    "#     out['angle'] = np.arctan(data['y'] / (data['x'] + 1e-5))\n",
    "#     out['c_angle'] = np.arctan(data['ycmc'] / (data['xcmc'] + 1e-5))\n",
    "#     out['q_energymc_ratio'] = data['q'] / (data['energymc'] + 1e-5)\n",
    "#     data['q_r'] = round(data['q'])\n",
    "#     for f in tqdm(['x', 'y', 'terror', 'q_r']):\n",
    "#         out[f + '_count'] = data[f].map(data[f].value_counts())\n",
    "#     for f in tqdm([\n",
    "#         ['x', 'y'], ['x', 'terror'], ['x', 'q_r'],\n",
    "#         ['y', 'terror'], ['y', 'q_r'], ['terror', 'q_r']\n",
    "#         ]):\n",
    "#         out['_'.join(f) + '_count'] = data.groupby(f)['hit_id'].transform('count')\n",
    "#     out['event_id_t_mm'] = data.groupby('event_id')['t'].transform('max') - data.groupby('event_id')['t'].transform('min')\n",
    "#     out['t_gap'] = (data['t']-data.groupby('event_id')['t'].transform('min')) / out['event_id_t_mm']\n",
    "#     out['event_id_q_sum'] = data.groupby('event_id')['q'].transform('sum')\n",
    "#     out['event_id_q_sum_energymc_diff'] = data['energymc'] - out['event_id_q_sum']\n",
    "#     out['event_id_q_sum_energymc_prop'] = out['event_id_q_sum'] / (data['energymc'] + 1e-5)\n",
    "    \n",
    "    out['x_'],out['y_'] = data['x'],data['y']\n",
    "    out['t_'] = data['t']\n",
    "    \n",
    "    data['x'],data['y'] = data['x']-data['xcmc'], data['y']-data['ycmc']\n",
    "    data['t'] = data['t'] - data.groupby('event_id')['t'].transform('mean')\n",
    "    \n",
    "    out['x'],out['y'] = data['x'],data['y']\n",
    "    out['t'] = data['t']\n",
    "    \n",
    "    data['tmin'],out['tmin'] = data['t']-data['terror'],data['t']-data['terror']\n",
    "    data['tmax'],out['tmax'] = data['t']+data['terror'],data['t']+data['terror']\n",
    "    out['x/t'] = data['x']/data['t']\n",
    "    out['y/t'] = data['y']/data['t']\n",
    "    data['d'] = (data['x']**2+data['y']**2)*np.sin(np.pi/2 - data['thetamc']*np.pi/180)\n",
    "    out['d'] = data['d']\n",
    "    out['d/t'] = out['d']/data['t']\n",
    "    out['d/tmin'] = out['d']/out['tmin']\n",
    "    out['d/tmax'] = out['d']/out['tmax']\n",
    "    \n",
    "    out['q'] = data['q']\n",
    "    out['q/t'] = data['q']/out['t']\n",
    "    out['q/tmin'] = data['q']/out['tmin']\n",
    "    out['q/tmax'] = data['q']/out['tmax']\n",
    "    \n",
    "    data_groupby_eid = data.groupby('event_id')\n",
    "    out['x_diff1'] = data_groupby_eid['x'].diff(1)\n",
    "    out['y_diff1'] = data_groupby_eid['y'].diff(1)\n",
    "    out['d_diff1'] = data_groupby_eid['d'].diff(1)\n",
    "    out['d_diff2'] = data_groupby_eid['d'].diff(2)\n",
    "    out['d_diff4'] = data_groupby_eid['d'].diff(4)\n",
    "    out['d_diff8'] = data_groupby_eid['d'].diff(8)\n",
    "    out['t_diff1'] = data_groupby_eid['t'].diff(1)\n",
    "    out['tmin_diff1'] = data_groupby_eid['tmin'].diff(1)\n",
    "    out['tmax_diff1'] = data_groupby_eid['tmax'].diff(1)\n",
    "    out['t_diff2'] = data_groupby_eid['t'].diff(2)\n",
    "    out['tmin_diff2'] = data_groupby_eid['tmin'].diff(2)\n",
    "    out['tmax_diff2'] = data_groupby_eid['tmax'].diff(2)\n",
    "    out['t_diff4'] = data_groupby_eid['t'].diff(4)\n",
    "    out['tmin_diff4'] = data_groupby_eid['tmin'].diff(4)\n",
    "    out['tmax_diff4'] = data_groupby_eid['tmax'].diff(4)\n",
    "    out['t_diff8'] = data_groupby_eid['t'].diff(8)\n",
    "    out['tmin_diff8'] = data_groupby_eid['tmin'].diff(8)\n",
    "    out['tmax_diff8'] = data_groupby_eid['tmax'].diff(8)\n",
    "    tmps = []\n",
    "    for name,group in data_groupby_eid:\n",
    "        tmps.append( (group['q'] / group['q'].sum()).values )\n",
    "    out['q_ratio'] = np.hstack(tmps).reshape(-1,1)\n",
    "    out['energy_q_ratio'] = out['q_ratio']*data['energymc']\n",
    "    \n",
    "    out['nhit'] = data['nhit']\n",
    "    out['nhitreal'] = data['nhitreal']\n",
    "    out['nhitreal_ratio'] = data['nhitreal'] / data['nhit']\n",
    "    out['energymc'] = data['energymc']\n",
    "    out['thetamc'] = data['thetamc']\n",
    "\n",
    "    out['terror'] = data['terror']\n",
    "    out['phimc'] = data['phimc']\n",
    "    out['xcmc'],out['ycmc'] = data['xcmc'],data['ycmc']\n",
    "    \n",
    "    # neighbour feature\n",
    "    print('Start to extract the neighbor features...')\n",
    "    tmps = []\n",
    "    for name,group in tqdm(data.groupby('event_id')):\n",
    "        tmps.append( neighbor2d_feature(group) )\n",
    "    feature_append1 = pd.DataFrame(np.vstack(tmps), columns=['neighbor1_num_2d', 'neighbor1_d_mean_2d', 'neighbor1_q_mean_2d', 'neighbor1_q_max_2d', 'neighbor1_d_min_2d', 'neighbor1_q_min_2d', \n",
    "                                                             'neighbor2_num_2d', 'neighbor2_d_mean_2d', 'neighbor2_q_mean_2d', 'neighbor2_q_max_2d', 'neighbor2_d_min_2d', 'neighbor2_q_min_2d',\n",
    "                                                             'neighbor3_num_2d', 'neighbor3_d_mean_2d', 'neighbor3_q_mean_2d', 'neighbor3_q_max_2d', 'neighbor3_d_min_2d', 'neighbor3_q_min_2d',],\n",
    "                                   dtype='float32')\n",
    "    tmps = []\n",
    "    for name,group in tqdm(data.groupby('event_id')):\n",
    "        tmps.append( neighbor3d_feature(group) )\n",
    "    feature_append2 = pd.DataFrame(np.vstack(tmps), columns=['neighbor1_num_3d', 'neighbor1_d_mean_3d', 'neighbor1_q_mean_3d', 'neighbor1_q_max_3d', 'neighbor1_d_min_3d', 'neighbor1_q_min_3d', \n",
    "                                                             'neighbor2_num_3d', 'neighbor2_d_mean_3d', 'neighbor2_q_mean_3d', 'neighbor2_q_max_3d', 'neighbor2_d_min_3d', 'neighbor2_q_min_3d',\n",
    "                                                             'neighbor3_num_3d', 'neighbor3_d_mean_3d', 'neighbor3_q_mean_3d', 'neighbor3_q_max_3d', 'neighbor3_d_min_3d', 'neighbor3_q_min_3d',],\n",
    "                                   dtype='float32')\n",
    "    \n",
    "    out = pd.concat([out,feature_append1,feature_append2], axis=1)\n",
    "    if 'flag' in data:\n",
    "        out['flag'] = data['flag']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取与数据保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5 = pd.HDFStore('data.h5','w', complevel=4, complib='blosc')\n",
    "\n",
    "#训练集特征\n",
    "print('train data...')\n",
    "data_train = pd.merge(pd.read_csv('data/train.csv'), pd.read_csv('data/event.csv')).sort_values(by=['event_id', 't']).reset_index(drop=True).astype('float32')\n",
    "train = feature_extract(data_train)\n",
    "h5['train'] = train\n",
    "\n",
    "#测试集特征\n",
    "print('test data...')\n",
    "data_test = pd.merge(pd.read_csv('data/test.csv'), pd.read_csv('data/event.csv')).sort_values(by=['event_id', 't']).reset_index(drop=True).astype('float32')\n",
    "test = feature_extract(data_test)\n",
    "h5['test'] = test\n",
    "\n",
    "del data_train,data_test,train,test\n",
    "gc.collect()\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "train = pd.read_hdf('data.h5', key='train').astype('float32')\n",
    "test  = pd.read_hdf('data.h5', key='test').astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签数据\n",
    "groupId = pd.read_csv('data/train.csv', usecols=['event_id','t']).sort_values(by=['event_id', 't']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初赛结束后新加的编码特征，结果有较大提升\n",
    "\n",
    "# train['detector'] = train['x_'].astype('str') + '_' + train['y_'].astype('str')\n",
    "# test['detector'] = test['x_'].astype('str') + '_' + test['y_'].astype('str')\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1029)\n",
    "# for f in tqdm(['detector', 'terror']):\n",
    "#     train[f + '_target_enc'] = 0\n",
    "#     test[f + '_target_enc'] = 0\n",
    "#     for i, (trn_idx, val_idx) in enumerate(skf.split(train, train['flag'])):\n",
    "#         trn_x = train[[f, 'flag']].iloc[trn_idx].reset_index(drop=True)\n",
    "#         val_x = train[[f]].iloc[val_idx].reset_index(drop=True)\n",
    "#         enc_df = trn_x.groupby(f, as_index=False)['flag'].agg({f + '_target_enc': 'mean'})\n",
    "#         val_x = val_x.merge(enc_df, on=f, how='left')\n",
    "#         test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "#         val_x[f + '_target_enc'] = val_x[f + '_target_enc'].fillna(train['flag'].mean())\n",
    "#         test_x[f + '_target_enc'] = test_x[f + '_target_enc'].fillna(train['flag'].mean())\n",
    "#         train.loc[val_idx, f + '_target_enc'] = val_x[f + '_target_enc'].values\n",
    "#         test[f + '_target_enc'] += test_x[f + '_target_enc'].values / skf.n_splits\n",
    "\n",
    "# del trn_x, val_x, enc_df,test_x\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型参数\n",
    "params = {\n",
    "            'learning_rate': 0.2,\n",
    "            'metric': 'auc',\n",
    "            'objective': 'binary',\n",
    "            'n_jobs': -1,\n",
    "            'seed': 1029,\n",
    "            'max_depth': -1,\n",
    "            'num_leaves': 64,\n",
    "            #'lambda_l1': 0.5,\n",
    "            'lambda_l2': 0.5\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "folds = GroupKFold(n_splits=3)\n",
    "oof_pred = np.zeros((len(train), ))\n",
    "y_pred = np.zeros((len(test), ))\n",
    "\n",
    "# drop_cols = ['flag', 'detector']\n",
    "drop_cols = ['flag']\n",
    "feaImportances = []\n",
    "for fold, (tr_ind, val_ind) in enumerate(folds.split(train, train['flag'], groupId['event_id'])):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    x_train, x_val = train.drop(drop_cols, axis=1).iloc[tr_ind].astype('float32'), train.drop(drop_cols, axis=1).iloc[val_ind].astype('float32')\n",
    "    y_train, y_val = train['flag'].iloc[tr_ind].astype('float32'), train['flag'].iloc[val_ind].astype('float32')\n",
    "    lgb_train,lgb_test = lgb.Dataset(x_train, y_train),lgb.Dataset(x_val, y_val)\n",
    "    model = lgb.train(params, \n",
    "                      lgb_train, \n",
    "                      num_boost_round=2000,\n",
    "                      early_stopping_rounds=35,\n",
    "                      valid_sets=[lgb_test],\n",
    "                      verbose_eval=100)\n",
    "    oof_pred[val_ind] = model.predict(x_val)\n",
    "    y_pred += model.predict(test.drop(drop_cols[1:], axis=1)) / folds.n_splits\n",
    "    feaImportances.append(pd.DataFrame({'feature': model.feature_name(), \n",
    "                                        'importance':model.feature_importance()}).sort_values('importance', ascending=False))\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_test\n",
    "    gc.collect()\n",
    "\n",
    "score = roc_auc_score(train['flag'], oof_pred) \n",
    "print('auc: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(pd.read_csv('data/test.csv').sort_values(by=['event_id', 't']), pd.read_csv('data/event.csv'))\n",
    "result['flag_pred'] = y_pred\n",
    "submission = result[['hit_id', 'flag_pred', 'event_id']]\n",
    "\n",
    "for eve_id,group in tqdm(result.groupby(['event_id'])[['nhitreal','flag_pred']]):\n",
    "    submission['flag_pred'].iloc[group['flag_pred'].nlargest(group['nhitreal'].iloc[0].astype('int')+5).index] = 1\n",
    "\n",
    "submission['flag_pred'][submission['flag_pred']!=1] = 0\n",
    "\n",
    "submission = submission.sort_values(by='hit_id')\n",
    "submission.to_csv(f'submit/submit_lgb_{score}.csv', index=False)\n",
    "submission.flag_pred.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
